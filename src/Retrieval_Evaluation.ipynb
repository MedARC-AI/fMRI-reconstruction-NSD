{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0bf040-6a16-464b-bb73-13dff44487fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Retrieval_Evaluation.ipynb to python\n",
      "[NbConvertApp] Writing 17695 bytes to Retrieval_Evaluation.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Code to convert this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Retrieval_Evaluation.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d437e335-a02b-4383-8fd1-cc80641b483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior, BrainDiffusionPriorOld, Voxel2StableDiffusionModel, VersatileDiffusionPriorNetwork\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4372359-c359-44c8-9910-5bdf50ef20d3",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9b8df7-abfd-479b-a5ab-db02ce27eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--data_path=/fsx/proj-medarc/fmri/natural-scenes-dataset', '--subj=1', '--model_name=test', '--model_name2=testing']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # Example use\n",
    "    jupyter_args = \"--data_path=/fsx/proj-medarc/fmri/natural-scenes-dataset \\\n",
    "                    --subj=1 \\\n",
    "                    --model_name=test\\\n",
    "                    --model_name2=testing\"\n",
    "    \n",
    "    jupyter_args = jupyter_args.split()\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    %autoreload 2 # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9db219-d363-4bdf-ae5e-b01a560fc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str,\n",
    "    help=\"name of 257x768 model, used for everything except LAION-5B retrieval\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name2\", type=str,\n",
    "    help=\"name of 1x768 model, used for LAION-5B retrieval\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-medarc/fmri/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored (see README)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8e373d-e598-4eed-b207-03090e520b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj 1 num_voxels 15724\n"
     ]
    }
   ],
   "source": [
    "subj = 1 #note: we only trained subjects 1 2 5 7, since they have data across full sessions\n",
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "print(\"subj\",subj,\"num_voxels\",num_voxels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1faed-7773-453a-9645-16953b04de66",
   "metadata": {},
   "source": [
    "# LAION-5B Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b394bd-3269-425b-b033-27264b8bee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([1, 3, 15724])\n",
      "img_input.shape torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "batch_size = val_batch_size = 1\n",
    "voxels_key = 'nsdgeneral.npy' # 1d inputs\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(val_batch_size, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "\n",
    "# check that your data loader is working\n",
    "for val_i, (voxel, img_input, coco) in enumerate(val_dl):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    print(\"img_input.shape\",img_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440746b6-a087-4f0b-aec1-688a499a9802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n",
      "ckpt_path ../train_logs/test/last.pth\n",
      "EPOCH:  0\n"
     ]
    }
   ],
   "source": [
    "out_dim = 257 * 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    ).to(device)\n",
    "\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ").to(device)\n",
    "\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a76b9f3-4217-43ea-9c2c-4e6e717efe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path ../train_logs/testing/last.pth\n",
      "EPOCH:  1\n"
     ]
    }
   ],
   "source": [
    "# CLS model\n",
    "out_dim = 768\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip_cls = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip_cls.requires_grad_(False)\n",
    "voxel2clip_cls.eval()\n",
    "\n",
    "diffusion_prior_cls = BrainDiffusionPriorOld.from_pretrained(\n",
    "    # kwargs for DiffusionPriorNetwork\n",
    "    dict(),\n",
    "    # kwargs for DiffusionNetwork\n",
    "    dict(\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=1000,\n",
    "        voxel2clip=voxel2clip_cls,\n",
    "    ),\n",
    "    voxel2clip_path=None,\n",
    ")\n",
    "\n",
    "outdir = f'../train_logs/{model_name2}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior_cls.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior_cls.eval().to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3cf95eb-68fb-42ef-8b02-c3f233cdab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-22 20:25:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/982 [00:00<?, ?it/s]/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  1%|â–Ž                                        | 6/982 [01:24<3:49:17, 14.10s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "retrieve = True\n",
    "plotting = False\n",
    "saving = True\n",
    "verbose = False\n",
    "imsize = 512\n",
    "\n",
    "all_brain_recons = None\n",
    "ind_include = np.arange(num_val)\n",
    "\n",
    "for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=len(ind_include))):\n",
    "    if val_i<np.min(ind_include):\n",
    "        continue\n",
    "    voxel = torch.mean(voxel,axis=1).to(device)\n",
    "    # voxel = voxel[:,0].to(device)\n",
    "    with torch.no_grad():        \n",
    "        grid, brain_recons, laion_best_picks, recon_img = utils.reconstruction(\n",
    "            img, voxel,\n",
    "            clip_extractor,\n",
    "            voxel2clip_cls = diffusion_prior_cls.voxel2clip,\n",
    "            diffusion_priors = diffusion_priors,\n",
    "            text_token = None,\n",
    "            n_samples_save = batch_size,\n",
    "            recons_per_sample = 0,\n",
    "            seed = seed,\n",
    "            retrieve = retrieve,\n",
    "            plotting = plotting,\n",
    "            verbose = verbose,\n",
    "            num_retrieved=16,\n",
    "        )\n",
    "            \n",
    "        if plotting:\n",
    "            plt.show()\n",
    "            # grid.savefig(f'evals/{model_name}_{val_i}.png')\n",
    "            # plt.close()\n",
    "            \n",
    "        brain_recons = brain_recons[laion_best_picks.astype(np.int8)]\n",
    "\n",
    "        if all_brain_recons is None:\n",
    "            all_brain_recons = brain_recons\n",
    "            all_images = img\n",
    "        else:\n",
    "            all_brain_recons = torch.vstack((all_brain_recons,brain_recons))\n",
    "            all_images = torch.vstack((all_images,img))\n",
    "            \n",
    "    if val_i>=np.max(ind_include):\n",
    "        break\n",
    "\n",
    "all_brain_recons = all_brain_recons.view(-1,3,imsize,imsize)\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "if saving:\n",
    "    torch.save(all_images,'all_images.pt')\n",
    "    torch.save(all_brain_recons,f'{model_name}_laion_retrievals_top16.pt')\n",
    "print(f'recon_path: {model_name}_laion_retrievals')\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f75bb-d6d6-475f-bf03-e507a56a7ab3",
   "metadata": {},
   "source": [
    "# Image/Brain retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1bc7cd-a709-4314-a357-56f9397db01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300 # same as used in mind_reader\n",
    "\n",
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "val_batch_size = 300\n",
    "val_loops = 30\n",
    "voxels_key = 'nsdgeneral.npy' # 1d inputs\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=True)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(val_batch_size, partial=False)\\\n",
    "    .with_epoch(val_loops)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "\n",
    "# check that your data loader is working\n",
    "for val_i, (voxel, img_input, coco) in enumerate(val_dl):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    print(\"img_input.shape\",img_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0a8d5-195e-4232-b5f1-14d56dfe85ca",
   "metadata": {},
   "source": [
    "### VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03a8f6-671d-4275-91f2-f9e6b71587d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 257 * 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    ).to(device)\n",
    "\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ").to(device)\n",
    "\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7e297-c8df-4a15-a119-6b99cb034b42",
   "metadata": {},
   "source": [
    "### Img variations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33c942-c0f1-4938-a8a5-f89beb5dd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dim = 768\n",
    "# clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=False, device=device)\n",
    "# voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "# voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "# voxel2clip.requires_grad_(False)\n",
    "# voxel2clip.eval()\n",
    "\n",
    "# out_dim = 768\n",
    "# depth = 6\n",
    "# dim_head = 64\n",
    "# heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "# timesteps = 1000\n",
    "\n",
    "\n",
    "# diffusion_prior = BrainDiffusionPriorOld.from_pretrained(\n",
    "#     # kwargs for DiffusionPriorNetwork\n",
    "#     dict(),\n",
    "#     # kwargs for DiffusionNetwork\n",
    "#     dict(\n",
    "#         condition_on_text_encodings=False,\n",
    "#         timesteps=timesteps,\n",
    "#         voxel2clip=voxel2clip,\n",
    "#     ),\n",
    "#     voxel2clip_path=None,\n",
    "# )\n",
    "\n",
    "# outdir = f'../train_logs/{model_name2}'\n",
    "# ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "# print(\"ckpt_path\",ckpt_path)\n",
    "# checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# print(\"EPOCH: \",checkpoint['epoch'])\n",
    "# diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "# diffusion_prior.eval().to(device)\n",
    "# diffusion_priors = [diffusion_prior]\n",
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c772a-880a-43b0-baa8-e17da89f8f68",
   "metadata": {},
   "source": [
    "## Retrieval on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d63b94-037c-416a-baa8-52a39c8d29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_correct_fwds, percent_correct_bwds = [], []\n",
    "percent_correct_fwd, percent_correct_bwd = None, None\n",
    "\n",
    "for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=val_loops)):\n",
    "    with torch.no_grad():\n",
    "        voxel = torch.mean(voxel,axis=1).to(device) # average across repetitions\n",
    "        # voxel = voxel[:,np.random.randint(3)].to(device) # random one of the single-trial samples\n",
    "\n",
    "        emb = clip_extractor.embed_image(img.to(device)).float() # CLIP-Image\n",
    "        \n",
    "        _, emb_ = diffusion_prior.voxel2clip(voxel.float()) # CLIP-Brain\n",
    "        \n",
    "        # flatten if necessary\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = emb_.reshape(len(emb_),-1)\n",
    "        \n",
    "        # l2norm \n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        \n",
    "        labels = torch.arange(len(emb)).to(device)\n",
    "        bwd_sim = utils.batchwise_cosine_similarity(emb,emb_)  # clip, brain\n",
    "        fwd_sim = utils.batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "\n",
    "        assert len(bwd_sim) == batch_size\n",
    "        \n",
    "        percent_correct_fwds = np.append(percent_correct_fwds, utils.topk(fwd_sim, labels,k=1).item())\n",
    "        percent_correct_bwds = np.append(percent_correct_bwds, utils.topk(bwd_sim, labels,k=1).item())\n",
    "            \n",
    "        if val_i==0:\n",
    "            print(\"Loop 0:\",percent_correct_fwds, percent_correct_bwds)\n",
    "            \n",
    "percent_correct_fwd = np.mean(percent_correct_fwds)\n",
    "fwd_sd = np.std(percent_correct_fwds) / np.sqrt(len(percent_correct_fwds))\n",
    "fwd_ci = stats.norm.interval(0.95, loc=percent_correct_fwd, scale=fwd_sd)\n",
    "\n",
    "percent_correct_bwd = np.mean(percent_correct_bwds)\n",
    "bwd_sd = np.std(percent_correct_bwds) / np.sqrt(len(percent_correct_bwds))\n",
    "bwd_ci = stats.norm.interval(0.95, loc=percent_correct_bwd, scale=bwd_sd)\n",
    "\n",
    "print(f\"fwd percent_correct: {percent_correct_fwd:.4f} 95% CI: [{fwd_ci[0]:.4f},{fwd_ci[1]:.4f}]\")\n",
    "print(f\"bwd percent_correct: {percent_correct_bwd:.4f} 95% CI: [{bwd_ci[0]:.4f},{bwd_ci[1]:.4f}]\")\n",
    "\n",
    "fwd_sim = np.array(fwd_sim.cpu())\n",
    "bwd_sim = np.array(bwd_sim.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f99b4-8c11-4345-a6ae-7e481873c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBJ 1\n",
    "# fwd percent_correct: 0.9718 95% CI: [0.9698,0.9737]\n",
    "# bwd percent_correct: 0.9468 95% CI: [0.9435,0.9500]\n",
    "\n",
    "# SUBJ 2\n",
    "# fwd percent_correct: 0.9710 95% CI: [0.9693,0.9727]\n",
    "# bwd percent_correct: 0.9386 95% CI: [0.9331,0.9440]\n",
    "\n",
    "# SUBJ 5\n",
    "# fwd percent_correct: 0.9067 95% CI: [0.9010,0.9123]\n",
    "# bwd percent_correct: 0.8573 95% CI: [0.8496,0.8651]\n",
    "\n",
    "# SUBJ 7\n",
    "# fwd percent_correct: 0.8941 95% CI: [0.8887,0.8995]\n",
    "# bwd percent_correct: 0.8582 95% CI: [0.8529,0.8636]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446f4f5-b270-4446-8220-6d2a6dfda7ec",
   "metadata": {},
   "source": [
    "## Image retrieval visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b819b5b-98f3-4ed6-943a-0cb1c9d05be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Given Brain embedding, find correct Image embedding\")\n",
    "fig, ax = plt.subplots(nrows=4, ncols=6, figsize=(11,12))\n",
    "for trial in range(4):\n",
    "    ax[trial, 0].imshow(utils.torch_to_Image(img[trial]))\n",
    "    ax[trial, 0].set_title(\"original\\nimage\")\n",
    "    ax[trial, 0].axis(\"off\")\n",
    "    for attempt in range(5):\n",
    "        which = np.flip(np.argsort(fwd_sim[trial]))[attempt]\n",
    "        ax[trial, attempt+1].imshow(utils.torch_to_Image(img[which]))\n",
    "        ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "        ax[trial, attempt+1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878d6f4-b46b-41e1-a860-2c2bac327d95",
   "metadata": {},
   "source": [
    "### Zebra example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7dad4-6489-4672-8be8-ff0392ac9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # move to cpu to not OOM with all 982 samples\n",
    "clip_extractor0 = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "diffusion_prior=diffusion_prior.to(device)\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(982, partial=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=1)):\n",
    "    with torch.no_grad():\n",
    "        voxel = torch.mean(voxel,axis=1).to(device) # average across repetitions\n",
    "\n",
    "        emb = clip_extractor0.embed_image(img.to(device)).float() # CLIP-Image\n",
    "        \n",
    "        _, emb_ = diffusion_prior.voxel2clip(voxel.float()) # CLIP-Brain\n",
    "        \n",
    "        # flatten if necessary\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = emb_.reshape(len(emb_),-1)\n",
    "        \n",
    "        # l2norm \n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        \n",
    "        labels = torch.arange(len(emb)).to(device)\n",
    "        bwd_sim = utils.batchwise_cosine_similarity(emb,emb_)  # clip, brain\n",
    "        fwd_sim = utils.batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "        \n",
    "        if percent_correct_fwd is None:\n",
    "            cnt=len(fwd_sim)\n",
    "            percent_correct_fwd = utils.topk(fwd_sim, labels,k=1)\n",
    "            percent_correct_bwd = utils.topk(bwd_sim, labels,k=1)\n",
    "        else:\n",
    "            cnt+=len(fwd_sim)\n",
    "            percent_correct_fwd += utils.topk(fwd_sim, labels,k=1)\n",
    "            percent_correct_bwd += utils.topk(bwd_sim, labels,k=1)\n",
    "            \n",
    "print(percent_correct_fwd, percent_correct_bwd)\n",
    "fwd_sim = fwd_sim.numpy()\n",
    "bwd_sim = bwd_sim.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49022d7b-5243-4de0-91a2-5bb76a56cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_indices = [32,33]#[891, 892, 893, 863, 833, 652, 516, 512, 498, 451, 331, 192, 129, 66] # 10\n",
    "print(\"# zebras =\", len(zebra_indices))\n",
    "fig, ax = plt.subplots(nrows=2, ncols=6, figsize=(11,5))\n",
    "for trial,t in enumerate(zebra_indices[:2]):\n",
    "    ax[trial, 0].imshow(utils.torch_to_Image(img_input[t]))\n",
    "    ax[trial, 0].set_title(\"Original image\")\n",
    "    ax[trial, 0].axis(\"off\")\n",
    "    for attempt in range(5):\n",
    "        which = np.flip(np.argsort(fwd_sim[t]))[attempt]\n",
    "        ax[trial, attempt+1].imshow(utils.torch_to_Image(img_input[which]))\n",
    "        ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "        ax[trial, attempt+1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff8a59-6ce6-4909-80ce-31a05dc10e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zebra_indices[:2]:\n",
    "    display(utils.torch_to_Image(img_input[z]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101abb0-f840-471d-8c85-0783348356c0",
   "metadata": {},
   "source": [
    "## Brain retrieval visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb7847-75a2-401e-9c9b-68385d4a9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Given Image embedding, find correct Brain embedding\")\n",
    "fig, ax = plt.subplots(nrows=4, ncols=6, figsize=(11,12))\n",
    "for trial in range(4):\n",
    "    ax[trial, 0].imshow(utils.torch_to_Image(img[trial]))\n",
    "    ax[trial, 0].set_title(\"original\\nimage\")\n",
    "    ax[trial, 0].axis(\"off\")\n",
    "    for attempt in range(5):\n",
    "        which = np.flip(np.argsort(bwd_sim[trial]))[attempt]\n",
    "        ax[trial, attempt+1].imshow(utils.torch_to_Image(img[which]))\n",
    "        ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "        ax[trial, attempt+1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
